{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando as bibliotecas\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spSession = SparkSession.builder.master(\"local\").appName(\"ml-telecom\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os dados e gerando os RDD's\n",
    "treinoRDD = sc.textFile(\"projeto4_telecom_treino.csv\")\n",
    "testeRDD = sc.textFile(\"projeto4_telecom_teste.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "projeto4_telecom_teste.csv MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Colocando os RDDs em cache\n",
    "treinoRDD.cache()\n",
    "testeRDD.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo os cabeçalhos do arquivo\n",
    "treinoRDD2 = treinoRDD.filter(lambda x: \"account_length\" not in x)\n",
    "testeRDD2 = testeRDD.filter(lambda x: \"account_length\" not in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para gerar RDD com linhas usando a função Row\n",
    "def geraLinha(inputStr) :\n",
    "    attList = inputStr.split(\",\")\n",
    "    \n",
    "    #convertendo a variavel target para numerica\n",
    "    if(attList[20].replace('\"', '')==\"no\"):\n",
    "        target = 0\n",
    "    else:\n",
    "        target = 1\n",
    "       \n",
    "    # Criando a linha, limpando e convertendo os dados de string para float\n",
    "    linha = Row(id = attList[0].replace('\"', ''), state = attList[1].replace('\"', ''), account_length = float(attList[2]), \n",
    "                area_code = attList[3].replace('\"', ''), international_plan = attList[4].replace('\"', ''), \n",
    "                voice_mail_plan = attList[5].replace('\"', ''), number_vmail_messages = float(attList[6]), \n",
    "                total_day_minutes = float(attList[7]), total_day_calls = float(attList[8]), \n",
    "                total_day_charge = float(attList[9]), total_eve_minutes = float(attList[10]), \n",
    "                total_eve_calls = float(attList[11]), total_eve_charge = float(attList[12]), \n",
    "                total_night_minutes = float(attList[13]), total_night_calls = float(attList[14]), \n",
    "                total_night_charge = float(attList[15]), total_intl_minutes = float(attList[16]), \n",
    "                total_intl_calls = float(attList[17]), total_intl_charge = float(attList[18]), \n",
    "                number_customer_service_calls = float(attList[19]), churn = target) \n",
    "    return linha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cria RDD's com linhas \n",
    "treinoRDD3 = treinoRDD2.map(geraLinha)\n",
    "testeRDD3 = testeRDD2.map(geraLinha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria os Dataframes de treino e teste\n",
    "treinoDF = spSession.createDataFrame(treinoRDD3)\n",
    "testeDF = spSession.createDataFrame(testeRDD3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando novos atributos\n",
    "treinoDF = treinoDF.withColumn(\"md_day_minute\", F.col(\"total_day_minutes\")/F.col(\"total_day_calls\"))\n",
    "treinoDF = treinoDF.withColumn(\"md_eve_minute\", F.col(\"total_eve_minutes\")/F.col(\"total_eve_calls\"))\n",
    "treinoDF = treinoDF.withColumn(\"md_night_minute\", F.col(\"total_night_minutes\")/F.col(\"total_night_calls\"))\n",
    "treinoDF = treinoDF.withColumn(\"md_intl_minute\", F.col(\"total_intl_minutes\")/F.col(\"total_intl_calls\"))\n",
    "treinoDF = treinoDF.withColumn(\"total_ligacoes\", F.col(\"total_day_calls\") + F.col(\"total_eve_calls\") + F.col(\"total_night_calls\") + F.col(\"total_intl_calls\"))\n",
    "treinoDF = treinoDF.withColumn(\"total_cobrado\", F.col(\"total_day_charge\") + F.col(\"total_eve_charge\") + F.col(\"total_night_charge\") + F.col(\"total_intl_charge\"))\n",
    "testeDF = testeDF.withColumn(\"md_day_minute\", F.col(\"total_day_minutes\")/F.col(\"total_day_calls\"))\n",
    "testeDF = testeDF.withColumn(\"md_eve_minute\", F.col(\"total_eve_minutes\")/F.col(\"total_eve_calls\"))\n",
    "testeDF = testeDF.withColumn(\"md_night_minute\", F.col(\"total_night_minutes\")/F.col(\"total_night_calls\"))\n",
    "testeDF = testeDF.withColumn(\"md_intl_minute\", F.col(\"total_intl_minutes\")/F.col(\"total_intl_calls\"))\n",
    "testeDF = testeDF.withColumn(\"total_ligacoes\", F.col(\"total_day_calls\") + F.col(\"total_eve_calls\") + F.col(\"total_night_calls\") + F.col(\"total_intl_calls\"))\n",
    "testeDF = testeDF.withColumn(\"total_cobrado\", F.col(\"total_day_charge\") + F.col(\"total_eve_charge\") + F.col(\"total_night_charge\") + F.col(\"total_intl_charge\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prenhendo os valores missing gerados pelos cálculos\n",
    "treinoDF = treinoDF.na.fill(0,[\"md_day_minute\",\"md_eve_minute\",\"md_night_minute\",\"md_intl_minute\"])\n",
    "testeDF = testeDF.na.fill(0,[\"md_day_minute\",\"md_eve_minute\",\"md_night_minute\",\"md_intl_minute\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#excluindo os atributos que possuem alta correlação\n",
    "treinoDF = treinoDF.drop(\"total_day_minutes\", \"total_eve_minutes\", \"total_night_minutes\", \"total_intl_minutes\")\n",
    "testeDF = testeDF.drop(\"total_day_minutes\", \"total_eve_minutes\", \"total_night_minutes\", \"total_intl_minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando RDD's temporarios para os dados, caso seja necessário voltar a essa etapa do processo\n",
    "treinoRDD_tmp  = treinoDF.rdd\n",
    "testeRDD_tmp  = testeDF.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumarizando os dados e extraindo o minimo e maximo de cada atributo para normalização\n",
    "estats = treinoDF.select(\"account_length\",\"number_customer_service_calls\",\"number_vmail_messages\",\"total_day_calls\",\"total_day_charge\",\"total_eve_calls\",\"total_eve_charge\",\"total_intl_calls\",\"total_intl_charge\",\"total_night_calls\",\"total_night_charge\",\"churn\", \"total_ligacoes\", \"total_cobrado\",\"md_day_minute\",\"md_eve_minute\",\"md_night_minute\",\"md_intl_minute\").describe().toPandas()\n",
    "minimo = estats.iloc[3,1:19].values.tolist()\n",
    "maximo = estats.iloc[4,1:19].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colocando  minimo e maximo em variáves do tipo Broadcast\n",
    "bc_minimo = sc.broadcast(minimo)\n",
    "bc_maximo = sc.broadcast(maximo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcao para normalização dos dados\n",
    "def normaliza(inRow) :\n",
    "    global bc_minimo\n",
    "    global bc_maximo\n",
    "    \n",
    "    minArray = bc_minimo.value\n",
    "    maxArray = bc_maximo.value\n",
    "    \n",
    "    rowDict = inRow.asDict()\n",
    "    retArray = {}\n",
    "    \n",
    "    i=0\n",
    "    for k in rowDict:\n",
    "        if(k==\"churn\"):\n",
    "            retArray[k] = float(inRow[i])\n",
    "        else:\n",
    "            retArray[k] = (float(inRow[i]) - float(minArray[i])) / (float(maxArray[i])-float(minArray[i]) )\n",
    "        i=i+1\n",
    "    return Row(**retArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aplicando normalização\n",
    "treinoRDD4 = treinoDF.select(\"account_length\",\"number_customer_service_calls\",\"number_vmail_messages\",\"total_day_calls\",\"total_day_charge\",\"total_eve_calls\",\"total_eve_charge\",\"total_intl_calls\",\"total_intl_charge\",\"total_night_calls\",\"total_night_charge\",\"churn\", \"total_ligacoes\", \"total_cobrado\",\"md_day_minute\",\"md_eve_minute\",\"md_night_minute\",\"md_intl_minute\").rdd.map(normaliza)\n",
    "treinoDF = spSession.createDataFrame(treinoRDD4)\n",
    "testeRDD4 = testeDF.select(\"account_length\",\"number_customer_service_calls\",\"number_vmail_messages\",\"total_day_calls\",\"total_day_charge\",\"total_eve_calls\",\"total_eve_charge\",\"total_intl_calls\",\"total_intl_charge\",\"total_night_calls\",\"total_night_charge\",\"churn\", \"total_ligacoes\", \"total_cobrado\",\"md_day_minute\",\"md_eve_minute\",\"md_night_minute\",\"md_intl_minute\").rdd.map(normaliza)\n",
    "testeDF = spSession.createDataFrame(testeRDD4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcão de SMOTE para os dados de treino, esta função irá balancear os dados de treino, pois a variável target (churn), possuí\n",
    "#mais valores 0 que 1, sendo assim, serão criados dados sintéticos com o valor da variável igual a 1, buscando minimizar o \n",
    "#erro principalmente nas previsões dos registros que contém target=1 (minimizar erro tipo 2)\n",
    "import random\n",
    "from sklearn import neighbors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#funcao que vetoriza os dados, criando um atributo com um vetor contendo as variáveis preditoras e outro atributo \n",
    "#com a variável target, modelo de dados utilizado pelos algoritmos de machine learning do spark\n",
    "def vectorizerFunction(dataInput, TargetFieldName):\n",
    "    if(dataInput.select(TargetFieldName).distinct().count() != 2):\n",
    "        raise ValueError(\"Target field must have only 2 distinct classes\")\n",
    "    columnNames = list(dataInput.columns)\n",
    "    columnNames.remove(TargetFieldName)\n",
    "    dataInput = dataInput.select((','.join(columnNames)+','+TargetFieldName).split(','))\n",
    "    assembler=VectorAssembler(inputCols = columnNames, outputCol = 'features')\n",
    "    pos_vectorized = assembler.transform(dataInput)\n",
    "    vectorized = pos_vectorized.select('features',TargetFieldName).withColumn('label',pos_vectorized[TargetFieldName]).drop(TargetFieldName)\n",
    "    return vectorized\n",
    "\n",
    "#função que cria os dados sintéticos, balanceando os dados de treino na variável target\n",
    "def SmoteSampling(vectorized, k = 5, minorityClass = 1, majorityClass = 0, percentageOver = 200, percentageUnder = 100):\n",
    "    if(percentageUnder > 100|percentageUnder < 10):\n",
    "        raise ValueError(\"Percentage Under must be in range 10 - 100\");\n",
    "    if(percentageOver < 100):\n",
    "        raise ValueError(\"Percentage Over must be in at least 100\");\n",
    "    dataInput_min = vectorized[vectorized['label'] == minorityClass]\n",
    "    dataInput_maj = vectorized[vectorized['label'] == majorityClass]\n",
    "    feature = dataInput_min.select('features')\n",
    "    feature = feature.rdd\n",
    "    feature = feature.map(lambda x: x[0])\n",
    "    feature = feature.collect()\n",
    "    feature = np.asarray(feature)\n",
    "    nbrs = neighbors.NearestNeighbors(n_neighbors=k, algorithm='auto').fit(feature)\n",
    "    neighbours =  nbrs.kneighbors(feature)\n",
    "    gap = neighbours[0]\n",
    "    neighbours = neighbours[1]\n",
    "    min_rdd = dataInput_min.drop('label').rdd\n",
    "    pos_rddArray = min_rdd.map(lambda x : list(x))\n",
    "    pos_ListArray = pos_rddArray.collect()\n",
    "    min_Array = list(pos_ListArray)\n",
    "    newRows = []\n",
    "    nt = len(min_Array)\n",
    "    nexs = percentageOver/100\n",
    "    for i in range(nt):\n",
    "        for j in range(int(nexs)):\n",
    "            neigh = random.randint(1,k)\n",
    "            difs = min_Array[neigh][0] - min_Array[i][0]\n",
    "            newRec = (min_Array[i][0]+random.random()*difs)\n",
    "            newRows.insert(0,(newRec))\n",
    "    newData_rdd = sc.parallelize(newRows)\n",
    "    newData_rdd_new = newData_rdd.map(lambda x: Row(features = x, label = 1))\n",
    "    new_data = newData_rdd_new.toDF()\n",
    "    new_data_minor = dataInput_min.unionAll(new_data)\n",
    "    new_data_major = dataInput_maj.sample(False, (float(percentageUnder)/float(100)))\n",
    "    return new_data_major.unionAll(new_data_minor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aplicando a técnica de SMOTE e vetorização na base de treino\n",
    "treinoDF = SmoteSampling(vectorizerFunction(treinoDF.select(\"account_length\",\"number_customer_service_calls\",\"number_vmail_messages\",\"total_day_calls\",\"total_day_charge\",\"total_eve_calls\",\"total_eve_charge\",\"total_intl_calls\",\"total_intl_charge\",\"total_night_calls\",\"total_night_charge\",\"churn\", \"total_ligacoes\", \"total_cobrado\",\"md_day_minute\",\"md_eve_minute\",\"md_night_minute\",\"md_intl_minute\"), \"churn\"), percentageOver=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vetorizando os dados de teste (nos dados de teste não há necessidade de criar dados sintéticos, pois serão usados apenas\n",
    "#para testar a acurácia do modelo já teinado)\n",
    "testeDF = vectorizerFunction(testeDF.select(\"account_length\",\"number_customer_service_calls\",\"number_vmail_messages\",\"total_day_calls\",\"total_day_charge\",\"total_eve_calls\",\"total_eve_charge\",\"total_intl_calls\",\"total_intl_charge\",\"total_night_calls\",\"total_night_charge\",\"churn\", \"total_ligacoes\", \"total_cobrado\",\"md_day_minute\",\"md_eve_minute\",\"md_night_minute\",\"md_intl_minute\"), \"churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000001FEEF82CDA0>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEy9JREFUeJzt3XGQ3OV93/H3x8i4RBCDq/iGCMWiU3kaHKaY3gAZzzTHkGKhzlh2J+5ASRA2U3kc6DgpzUR2/8Bjxq3dlLg1dbHlQQPuEMu0iSsNqGVU6hvHSeQgEorAhOGCFZClojjCsmUap3K//WN/Cos46fb27nY5P+/XzM7+9tnnt8/zvTv2o9/z++2SqkKS1J7XjXsCkqTxMAAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAKh5SfYn+fkB+lWSvz3kGEPvKy0VA0CSGmUASFKjDACpk+SyJH+Q5DtJDiX5D0nOPKnbhiTPJvl2kt9I8rq+/d+f5KkkLyZ5KMlbRlyCNC8GgPSyHwK/CqwCfha4Cvjlk/q8B5gELgU2Au8HSPJu4CPAPwJ+Avhd4IsjmbU0JANA6lTVo1W1p6qOV9V+4HPAz53U7ZNVdaSqngP+HXBd1/4B4F9X1VNVdRz4V8AlHgXotcwAkDpJ3prkgST/O8l36b2Jrzqp2/N9238G/GS3/Rbg33fLR98BjgABVi/1vKVhGQDSy+4C/gRYV1U/Tm9JJyf1WdO3/VPAwW77eeADVXVu3+2sqvr9JZ+1NCQDQHrZOcB3gWNJ/g7wwVn6/FqS85KsAT4EfKlr/yzw4SRvA0jyxiTvHcWkpWEZANLL/gXwT4DvAZ/n5Tf3fjuAR4HHgAeBuwGq6svAJ4Ht3fLRE8A1I5izNLT4P4SRpDZ5BCBJjTIAJKlRBoAkNcoAkKRGrRj3BE5n1apVtXbt2qH3//73v8/KlSsXb0LLQGs1t1YvWHMrFlLzo48++u2q+om5+r2mA2Dt2rXs3bt36P2np6eZmppavAktA63V3Fq9YM2tWEjNSf5skH4uAUlSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqNe058EXqh93zrKjVseHPm4+z/xD0c+piTNl0cAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY2aMwCSrEnylSRPJXkyyYe69o8m+VaSx7rbhr59PpxkJsnTSd7Z176+a5tJsmVpSpIkDWLFAH2OA7dW1R8lOQd4NMnu7rlPVdW/7e+c5CLgWuBtwE8C/yPJW7unPwP8A+AA8EiSnVX1jcUoRJI0P3MGQFUdAg51299L8hSw+jS7bAS2V9UPgG8mmQEu656bqapnAZJs7/oaAJI0BoMcAfy1JGuBtwNfB94B3JLkBmAvvaOEF+mFw56+3Q7wcmA8f1L75bOMsRnYDDAxMcH09PR8pvgKE2fBrRcfH3r/YS1kzgt17NixsY4/aq3VC9bcilHUPHAAJDkb+G3gV6rqu0nuAm4Hqru/A3g/kFl2L2Y/31CvaqjaCmwFmJycrKmpqUGn+Cp33reDO/bNK+MWxf7rp0Y+5gnT09Ms5Ge23LRWL1hzK0ZR80DvjkleT+/N/76q+h2Aqnqh7/nPAw90Dw8Aa/p2vwA42G2fql2SNGKDXAUU4G7gqar6zb728/u6vQd4otveCVyb5A1JLgTWAX8IPAKsS3JhkjPpnSjeuThlSJLma5AjgHcAvwTsS/JY1/YR4Lokl9BbxtkPfACgqp5Mcj+9k7vHgZur6ocASW4BHgLOALZV1ZOLWIskaR4GuQroa8y+rr/rNPt8HPj4LO27TrefJGl0Rn+GVJKWkbVbHhzLuPesX7nkY/hVEJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkho1ZwAkWZPkK0meSvJkkg917W9KsjvJM939eV17knw6yUySx5Nc2vdam7r+zyTZtHRlSZLmMsgRwHHg1qr6aeAK4OYkFwFbgIerah3wcPcY4BpgXXfbDNwFvcAAbgMuBy4DbjsRGpKk0ZszAKrqUFX9Ubf9PeApYDWwEbi363Yv8O5ueyPwherZA5yb5HzgncDuqjpSVS8Cu4H1i1qNJGlgK+bTOcla4O3A14GJqjoEvZBI8uau22rg+b7dDnRtp2o/eYzN9I4cmJiYYHp6ej5TfIWJs+DWi48Pvf+wFjLnhTp27NhYxx+11uoFax61cbyHwGhqHjgAkpwN/DbwK1X13SSn7DpLW52m/ZUNVVuBrQCTk5M1NTU16BRf5c77dnDHvnll3KLYf/3UyMc8YXp6moX8zJab1uoFax61G7c8OJZx71m/cslrHugqoCSvp/fmf19V/U7X/EK3tEN3f7hrPwCs6dv9AuDgadolSWMwyFVAAe4Gnqqq3+x7aidw4kqeTcCOvvYbuquBrgCOdktFDwFXJzmvO/l7ddcmSRqDQdZH3gH8ErAvyWNd20eATwD3J7kJeA54b/fcLmADMAO8BLwPoKqOJLkdeKTr97GqOrIoVUiS5m3OAKiqrzH7+j3AVbP0L+DmU7zWNmDbfCYoSVoafhJYkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjZozAJJsS3I4yRN9bR9N8q0kj3W3DX3PfTjJTJKnk7yzr3191zaTZMvilyJJmo9BjgDuAdbP0v6pqrqku+0CSHIRcC3wtm6f/5jkjCRnAJ8BrgEuAq7r+kqSxmTFXB2q6qtJ1g74ehuB7VX1A+CbSWaAy7rnZqrqWYAk27u+35j3jCVJi2LOADiNW5LcAOwFbq2qF4HVwJ6+Pge6NoDnT2q/fLYXTbIZ2AwwMTHB9PT00BOcOAtuvfj40PsPayFzXqhjx46NdfxRa61esOZRG8d7CIym5mED4C7gdqC6+zuA9wOZpW8x+1JTzfbCVbUV2AowOTlZU1NTQ04R7rxvB3fsW0jGDWf/9VMjH/OE6elpFvIzW25aqxesedRu3PLgWMa9Z/3KJa95qHfHqnrhxHaSzwMPdA8PAGv6ul4AHOy2T9UuSRqDoS4DTXJ+38P3ACeuENoJXJvkDUkuBNYBfwg8AqxLcmGSM+mdKN45/LQlSQs15xFAki8CU8CqJAeA24CpJJfQW8bZD3wAoKqeTHI/vZO7x4Gbq+qH3evcAjwEnAFsq6onF70aSdLABrkK6LpZmu8+Tf+PAx+fpX0XsGtes5MkLRk/CSxJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNWrOAEiyLcnhJE/0tb0pye4kz3T353XtSfLpJDNJHk9yad8+m7r+zyTZtDTlSJIGNcgRwD3A+pPatgAPV9U64OHuMcA1wLruthm4C3qBAdwGXA5cBtx2IjQkSeMxZwBU1VeBIyc1bwTu7bbvBd7d1/6F6tkDnJvkfOCdwO6qOlJVLwK7eXWoSJJGaMWQ+01U1SGAqjqU5M1d+2rg+b5+B7q2U7W/SpLN9I4emJiYYHp6esgpwsRZcOvFx4fef1gLmfNCHTt2bKzjj1pr9YI1j9o43kNgNDUPGwCnklna6jTtr26s2gpsBZicnKypqamhJ3PnfTu4Y99ilzi3/ddPjXzME6anp1nIz2y5aa1esOZRu3HLg2MZ9571K5e85mGvAnqhW9qhuz/ctR8A1vT1uwA4eJp2SdKYDBsAO4ETV/JsAnb0td/QXQ10BXC0Wyp6CLg6yXndyd+ruzZJ0pjMuT6S5IvAFLAqyQF6V/N8Arg/yU3Ac8B7u+67gA3ADPAS8D6AqjqS5Hbgka7fx6rq5BPLkqQRmjMAquq6Uzx11Sx9C7j5FK+zDdg2r9lJkpaMnwSWpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhq1oABIsj/JviSPJdnbtb0pye4kz3T353XtSfLpJDNJHk9y6WIUIEkazmIcAVxZVZdU1WT3eAvwcFWtAx7uHgNcA6zrbpuBuxZhbEnSkJZiCWgjcG+3fS/w7r72L1TPHuDcJOcvwfiSpAGkqobfOfkm8CJQwOeqamuS71TVuX19Xqyq85I8AHyiqr7WtT8M/HpV7T3pNTfTO0JgYmLi723fvn3o+R0+cpQX/s/Quw/t4tVvHP2gnWPHjnH22WePbfxRa61esOZR2/eto2MZ98I3njF0zVdeeeWjfasyp7RiqFd/2Tuq6mCSNwO7k/zJafpmlrZXpU9VbQW2AkxOTtbU1NTQk7vzvh3csW+hJc7f/uunRj7mCdPT0yzkZ7bctFYvWPOo3bjlwbGMe8/6lUte84KWgKrqYHd/GPgycBnwwomlne7+cNf9ALCmb/cLgIMLGV+SNLyhAyDJyiTnnNgGrgaeAHYCm7pum4Ad3fZO4IbuaqArgKNVdWjomUuSFmQh6yMTwJeTnHid36qq/57kEeD+JDcBzwHv7frvAjYAM8BLwPsWMLYkaYGGDoCqehb4u7O0/wVw1SztBdw87HiSpMXlJ4ElqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUqJEHQJL1SZ5OMpNky6jHlyT1jDQAkpwBfAa4BrgIuC7JRaOcgySpZ9RHAJcBM1X1bFX9FbAd2DjiOUiSgBUjHm818Hzf4wPA5f0dkmwGNncPjyV5egHjrQK+vYD9h5JPjnrEVxhLzWPUWr1gzU248pMLqvktg3QadQBklrZ6xYOqrcDWRRks2VtVk4vxWstFazW3Vi9YcytGUfOol4AOAGv6Hl8AHBzxHCRJjD4AHgHWJbkwyZnAtcDOEc9BksSIl4Cq6niSW4CHgDOAbVX15BIOuShLSctMazW3Vi9YcyuWvOZU1dy9JEk/cvwksCQ1ygCQpEYt+wCY66slkrwhyZe657+eZO3oZ7m4Bqj5nyf5RpLHkzycZKBrgl/LBv0KkSS/kKSSLPtLBgepOck/7n7XTyb5rVHPcbEN8Lf9U0m+kuSPu7/vDeOY52JJsi3J4SRPnOL5JPl09/N4PMmlizqBqlq2N3onkv8U+FvAmcD/Ai46qc8vA5/ttq8FvjTueY+g5iuBH+u2P9hCzV2/c4CvAnuAyXHPewS/53XAHwPndY/fPO55j6DmrcAHu+2LgP3jnvcCa/77wKXAE6d4fgPw3+h9huoK4OuLOf5yPwIY5KslNgL3dtv/BbgqyWwfSFsu5qy5qr5SVS91D/fQ+7zFcjboV4jcDvwb4C9HObklMkjN/xT4TFW9CFBVh0c8x8U2SM0F/Hi3/UaW+eeIquqrwJHTdNkIfKF69gDnJjl/scZf7gEw21dLrD5Vn6o6DhwF/uZIZrc0Bqm53030/gWxnM1Zc5K3A2uq6oFRTmwJDfJ7fivw1iS/l2RPkvUjm93SGKTmjwK/mOQAsAv4Z6OZ2tjM97/3eRn1V0Estjm/WmLAPsvJwPUk+UVgEvi5JZ3R0jttzUleB3wKuHFUExqBQX7PK+gtA03RO8r73SQ/U1XfWeK5LZVBar4OuKeq7kjys8B/6mr+f0s/vbFY0vev5X4EMMhXS/x1nyQr6B02nu6Q67VuoK/TSPLzwL8E3lVVPxjR3JbKXDWfA/wMMJ1kP7210p3L/ETwoH/bO6rq/1bVN4Gn6QXCcjVIzTcB9wNU1R8Af4PeF8X9qFrSr89Z7gEwyFdL7AQ2ddu/APzP6s6uLFNz1twth3yO3pv/cl8XhjlqrqqjVbWqqtZW1Vp65z3eVVV7xzPdRTHI3/Z/pXfCnySr6C0JPTvSWS6uQWp+DrgKIMlP0wuAPx/pLEdrJ3BDdzXQFcDRqjq0WC++rJeA6hRfLZHkY8DeqtoJ3E3vMHGG3r/8rx3fjBduwJp/Azgb+M/d+e7nqupdY5v0Ag1Y84+UAWt+CLg6yTeAHwK/VlV/Mb5ZL8yANd8KfD7Jr9JbCrlxOf+DLskX6S3hrerOa9wGvB6gqj5L7zzHBmAGeAl436KOv4x/dpKkBVjuS0CSpCEZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlR/x93GMJjfMxjdQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#verificando o balanceamento da variável target após SMOTE\n",
    "treinoDF.select(\"label\").toPandas().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribuição da variável target está mais equilibrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicação de Machine Learning\n",
    "Serão testados alguns algoritmos de machine learning para o problema de classificação, pelo fato dos dados serem desbalanceados, a acurácia dos algoritmos será medida a partir da area under the ROC curve. O objetivo é um score mínimo de 0.80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7437351499851506"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aplicando regressão logistica na base de treino e testando o modelo\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "#treinando o modelo\n",
    "lr = LogisticRegression()\n",
    "lr_mdl = lr.fit(treinoDF)\n",
    "\n",
    "#previsão dados de teste\n",
    "pred = lr_mdl.transform(testeDF)\n",
    "\n",
    "#avaliando a performance do modelo\n",
    "evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "evaluator.evaluate(pred)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando uma base com as variáveis mais importantes\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "\n",
    "selector = ChiSqSelector(numTopFeatures=10, outputCol=\"bst_features\")\n",
    "selector_mdl = selector.fit(treinoDF)\n",
    "result = selector_mdl.transform(treinoDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7333061083061083"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#treinando o modelo com as variáveis mais importantes\n",
    "lr = LogisticRegression(featuresCol='bst_features')\n",
    "lr_mdl = lr.fit(result)\n",
    "\n",
    "#previsão dados de teste\n",
    "pred = lr_mdl.transform(selector_mdl.transform(testeDF))\n",
    "\n",
    "#avaliando a performance do modelo\n",
    "evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "evaluator.evaluate(pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seleção de variáveis piorou o resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7175449831699827"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#treinando o modelo com outros parâmetros\n",
    "lr = LogisticRegression(regParam=0.2, elasticNetParam=0.5)\n",
    "lr_mdl = lr.fit(treinoDF)\n",
    "\n",
    "#previsão dados de teste\n",
    "pred = lr_mdl.transform(testeDF)\n",
    "\n",
    "#avaliando a performance do modelo\n",
    "evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "evaluator.evaluate(pred)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentar os parâmetros de regularização e elasticNet também fez com que a performance do modelo piorasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7500572344322344"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testando o algoritmo decision tree\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "#treinando o modelo\n",
    "dt = DecisionTreeClassifier()\n",
    "dt_mdl = dt.fit(treinoDF)\n",
    "\n",
    "#prevendo os dados de teste\n",
    "pred = dt_mdl.transform(testeDF)\n",
    "\n",
    "#avaliando a performance do modelo\n",
    "evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "evaluator.evaluate(pred)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo de decision tree teve performance melhor que regressão logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8274289055539054"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testando o algoritmo random forest\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "#treinando o modelo\n",
    "rf = RandomForestClassifier()\n",
    "rf_mdl = rf.fit(treinoDF)\n",
    "\n",
    "#prevendo os dados de teste\n",
    "pred = rf_mdl.transform(testeDF)\n",
    "\n",
    "#avaliando a performance do modelo\n",
    "evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "evaluator.evaluate(pred)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest teve a melhor perfomance até o momento, apesar do objetivo de perfomance ter sido superado por esse algoritmo, ainda serão testados outros métodos buscando uma melhor acurácia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36215473715473734"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testando o algortimo NaiveBayes\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "#treinando o modelo\n",
    "nb = NaiveBayes()\n",
    "nb_mdl = nb.fit(treinoDF)\n",
    "\n",
    "#previsão dados de teste\n",
    "pred = nb_mdl.transform(testeDF)\n",
    "\n",
    "#avaliando a performance do modelo\n",
    "evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "evaluator.evaluate(pred)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo Naive Bayes não apresentou uma boa acurácia. Por apresentar a melhor acurácia entre os algoritmos testados, os próximos testes serão feitos utilizando o algoritmo random forest, primeiramente serão feitas modificações na base de treinos, buscando aquelas que apresentarem a melhor performance, após isso, com o modelo de dados que melhor performou, serão ajustados os hyperparâmetros do algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testando os dados sem SMOTE\n",
    "\n",
    "#usando os RRD's 4 (antes da aplicação do SMOTE)\n",
    "treinoDF = spSession.createDataFrame(treinoRDD4)\n",
    "testeDF = spSession.createDataFrame(testeRDD4)\n",
    "\n",
    "#vetorizando os dados\n",
    "treinoDF = vectorizerFunction(treinoDF.select(\"account_length\",\"number_customer_service_calls\",\"number_vmail_messages\",\"total_day_calls\",\"total_day_charge\",\"total_eve_calls\",\"total_eve_charge\",\"total_intl_calls\",\"total_intl_charge\",\"total_night_calls\",\"total_night_charge\",\"churn\", \"total_ligacoes\", \"total_cobrado\",\"md_day_minute\",\"md_eve_minute\",\"md_night_minute\",\"md_intl_minute\"), \"churn\")\n",
    "testeDF = vectorizerFunction(testeDF.select(\"account_length\",\"number_customer_service_calls\",\"number_vmail_messages\",\"total_day_calls\",\"total_day_charge\",\"total_eve_calls\",\"total_eve_charge\",\"total_intl_calls\",\"total_intl_charge\",\"total_night_calls\",\"total_night_charge\",\"churn\", \"total_ligacoes\", \"total_cobrado\",\"md_day_minute\",\"md_eve_minute\",\"md_night_minute\",\"md_intl_minute\"), \"churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.887009949509952"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aplicando o algoritmo\n",
    "\n",
    "#treinando o modelo\n",
    "rf = RandomForestClassifier()\n",
    "rf_mdl = rf.fit(treinoDF)\n",
    "\n",
    "#prevendo os dados de teste\n",
    "pred = rf_mdl.transform(testeDF)\n",
    "\n",
    "#avaliando a performance do modelo\n",
    "evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "evaluator.evaluate(pred)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo apresentou uma boa melhora sem o uso de SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testando os dados sem normalização\n",
    "\n",
    "#usando os RDDs temporarios (antes da aplicação da normalização)\n",
    "treinoDF = spSession.createDataFrame(treinoRDD_tmp)\n",
    "testeDF = spSession.createDataFrame(testeRDD_tmp)\n",
    "\n",
    "#vetorizando os dados\n",
    "treinoDF = vectorizerFunction(treinoDF.select(\"account_length\",\"number_customer_service_calls\",\"number_vmail_messages\",\"total_day_calls\",\"total_day_charge\",\"total_eve_calls\",\"total_eve_charge\",\"total_intl_calls\",\"total_intl_charge\",\"total_night_calls\",\"total_night_charge\",\"churn\", \"total_ligacoes\", \"total_cobrado\",\"md_day_minute\",\"md_eve_minute\",\"md_night_minute\",\"md_intl_minute\"), \"churn\")\n",
    "testeDF = vectorizerFunction(testeDF.select(\"account_length\",\"number_customer_service_calls\",\"number_vmail_messages\",\"total_day_calls\",\"total_day_charge\",\"total_eve_calls\",\"total_eve_charge\",\"total_intl_calls\",\"total_intl_charge\",\"total_night_calls\",\"total_night_charge\",\"churn\", \"total_ligacoes\", \"total_cobrado\",\"md_day_minute\",\"md_eve_minute\",\"md_night_minute\",\"md_intl_minute\"), \"churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8870099495099523"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aplicando o algoritmo\n",
    "\n",
    "#treinando o modelo\n",
    "rf = RandomForestClassifier()\n",
    "rf_mdl = rf.fit(treinoDF)\n",
    "\n",
    "#prevendo os dados de teste\n",
    "pred = rf_mdl.transform(testeDF)\n",
    "\n",
    "#avaliando a performance do modelo\n",
    "evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "evaluator.evaluate(pred)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não houve diferenças no score utilizando os dados sem normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aplicando redução de dimensionalidade com PCA\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k = 6, inputCol = \"features\", outputCol = \"pcaFeatures\")\n",
    "pcaModel = pca.fit(treinoDF)\n",
    "pcaResult = pcaModel.transform(treinoDF).select(\"label\",\"pcaFeatures\")\n",
    "pcaTest =  pcaModel.transform(testeDF).select(\"label\",\"pcaFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.750146953271954"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aplicando o algoritmo\n",
    "\n",
    "#treinando o modelo\n",
    "rf = RandomForestClassifier(featuresCol = \"pcaFeatures\")\n",
    "rf_mdl = rf.fit(pcaResult)\n",
    "\n",
    "#prevendo os dados de teste\n",
    "pred = rf_mdl.transform(pcaTest)\n",
    "\n",
    "#avaliando a performance do modelo\n",
    "evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "evaluator.evaluate(pred)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduzir a dimensionalidade não melhorou a acurácia do modelo, pelo contrário, apresentou um score abaixo do anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testando outros parâmetros no algoritmo, o modelo será treinado com combinações de hyperparâmetros\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "#criando o estimador\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "#criando o grid de hyperparâmetros\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(rf.maxDepth, [5, 8, 12, 15])\\\n",
    "    .addGrid(rf.minInstancesPerNode, [1, 3, 5, 7])\\\n",
    "    .addGrid(rf.numTrees, [30, 50, 70, 100])\\\n",
    "    .addGrid(rf.subsamplingRate, [0.3, 0.5, 0.8, 1])\\\n",
    "    .build()\n",
    "    \n",
    "#treinando o modelo usando as combinações de hyperparâmetros, 80% dos dados serão usados para treino e 20% para testes\n",
    "tvs = TrainValidationSplit(estimator=rf,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=BinaryClassificationEvaluator(),\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "tv_model = tvs.fit(treinoDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8943019255519248"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prevendo os dados de teste\n",
    "pred = tv_model.transform(testeDF)\n",
    "\n",
    "#avaliando a performance do modelo\n",
    "evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "evaluator.evaluate(pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo até então teve o melhor resultado de todos os testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(17, {0: 0.0087, 1: 0.1849, 2: 0.1073, 3: 0.0087, 4: 0.1331, 5: 0.0072, 6: 0.0241, 7: 0.0076, 8: 0.0191, 9: 0.0071, 10: 0.0068, 11: 0.0049, 12: 0.4101, 13: 0.0246, 14: 0.0127, 15: 0.0101, 16: 0.0229})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#analisando a importancia dos atributos do melhor modelo\n",
    "tv_model.bestModel.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxDepth: 5\n",
      "minInstancesPerNode: 7\n",
      "numTrees: 50\n",
      "subsamplingRate: 0.8\n"
     ]
    }
   ],
   "source": [
    "#obtendo os parametros do melhor modelo\n",
    "print(\"maxDepth: \" + str(tv_model.bestModel._java_obj.getMaxDepth()))\n",
    "print(\"minInstancesPerNode: \" + str(tv_model.bestModel._java_obj.getMinInstancesPerNode()))\n",
    "print(\"numTrees: \"  + str(tv_model.bestModel._java_obj.getNumTrees()))\n",
    "print(\"subsamplingRate: \"  + str(tv_model.bestModel._java_obj.getSubsamplingRate()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando a base de dados com as melhores features\n",
    "#os atributos mais importantes são os seguintes (por indice) 12,1,4,2,13,6,16,8,14,15, também será incluido o atributo 17 ma \n",
    "#lista (variável target), criando a lista de atributos na ordem utilizada na transformação:\n",
    "list_attr = np.array([\"account_length\",\"number_customer_service_calls\",\"number_vmail_messages\",\"total_day_calls\",\"total_day_charge\",\"total_eve_calls\",\"total_eve_charge\",\"total_intl_calls\",\"total_intl_charge\",\"total_night_calls\",\"total_night_charge\",\"total_ligacoes\", \"total_cobrado\",\"md_day_minute\",\"md_eve_minute\",\"md_night_minute\",\"md_intl_minute\",\"churn\"])\n",
    "\n",
    "#recriando os dataframes originais\n",
    "new_treinoDF = spSession.createDataFrame(treinoRDD_tmp)\n",
    "new_testeDF = spSession.createDataFrame(testeRDD_tmp)\n",
    "\n",
    "#vetorizando os dados\n",
    "new_treinoDF = vectorizerFunction(new_treinoDF.select(list(list_attr[[12,1,4,2,13,6,16,8,14,15,17]])), \"churn\")\n",
    "new_testeDF = vectorizerFunction(new_testeDF.select(list(list_attr[[12,1,4,2,13,6,16,8,14,15,17]])), \"churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8889033264033266"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aplicando o algoritmo\n",
    "\n",
    "#treinando o modelo\n",
    "rf = RandomForestClassifier(labelCol=\"label\", maxDepth=5, minInstancesPerNode=7, numTrees=50, subsamplingRate=0.8)\n",
    "rf_mdl = rf.fit(new_treinoDF)\n",
    "\n",
    "#prevendo os dados de teste\n",
    "pred = rf_mdl.transform(new_testeDF)\n",
    "\n",
    "#avaliando a performance do modelo\n",
    "evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "evaluator.evaluate(pred)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seleção de variáveis piorou o modelo, portanto, o melhor modelo entre todos foi o random forest com os seguintes parâmetros: maxDepth: 5, minInstancesPerNode: 7, numTrees: 50, subsamplingRate: 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aplicando as predições com o melhor modelo nos dados de teste\n",
    "pred = tv_model.transform(testeDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|   84|\n",
      "|    0|       1.0|    8|\n",
      "|    0|       0.0| 1435|\n",
      "|    1|       1.0|  140|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#avaliando a matriz de confusão do modelo\n",
    "pred.groupBy(\"label\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "92 previsões erradas, 1.575 previsões corretas (94,48% de acerto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcao para extrair das previsões o valor previsto e a probabilidade de isso ocorrer\n",
    "def probaPrediction(rw):\n",
    "    pred = rw[\"prediction\"]\n",
    "    proba = rw[\"probability\"].values.item(int(pred))\n",
    "    return Row(predicao = pred, probabilidade = proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando um dataframe com as previsoes e as probabilidades\n",
    "predDF = spSession.createDataFrame(pred.rdd.map(probaPrediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando o data frame com os dados de teste\n",
    "new_testeDF = spSession.createDataFrame(testeRDD3)\n",
    "\n",
    "#será criado um id usando a função monotonically_increasing_id para a junção dos dois dataframes, porém essa função não gera \n",
    "#os dados de forma sequencial, portanto o id original será mantido, renomeado e após a junção das duas bases essa coluna \n",
    "#será novamente renomeada para o nome original\n",
    "new_testeDF = new_testeDF.withColumnRenamed(\"id\",\"id_old\")\n",
    "new_testeDF = new_testeDF.withColumn(\"idj\", F.monotonically_increasing_id())\n",
    "\n",
    "#colocando uma coluna ID nos dados de previsao para juntá-los\n",
    "predDF = predDF.withColumn(\"idj\", F.monotonically_increasing_id())\n",
    "\n",
    "#juntando com os dados de treino com as previsões\n",
    "testeDF_final = new_testeDF.join(predDF, on=\"idj\")\n",
    "#reordenando os dados\n",
    "testeDF_final = testeDF_final.orderBy(\"idj\")\n",
    "#excluindo a coluna utilizada para junção\n",
    "testeDF_final = testeDF_final.drop(\"idj\",\"id\")\n",
    "#retornando a coluna id original\n",
    "testeDF_final = testeDF_final.withColumnRenamed(\"id_old\",\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#salvando os dados com as previsões em arquivo\n",
    "testeDF_final.toPandas().to_csv('previsoes.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O arquivo previsoes.csv contem os dados originais de treino, as previsões e a probabilidade de acerto do modelo para cada previsão, o modelo aplicado acertou 94,48% das previsões, com score de area Under ROC de 0.8943."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
